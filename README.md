# ğŸ¤– Chatbot Local com Ollama (LLaMA 3)

Este Ã© um projeto de **chatbot local** feito em **Python**, utilizando o **Ollama** como backend de modelo de linguagem, com suporte ao modelo **LLaMA 3**. Tudo roda **localmente**, usando o seu **processador e placa de vÃ­deo**, sem necessidade de pagar por APIs como OpenAI ou Gemini.

---

## âœ¨ Principais CaracterÃ­sticas

- Roda totalmente **offline**, com modelos locais.
- Usa **LangChain** para integraÃ§Ã£o com o modelo.
- Armazena **usuÃ¡rios** e **conversas** localmente.
- Suporta mÃºltiplos usuÃ¡rios e histÃ³rico de conversa.
- Arquitetura modular e de fÃ¡cil manutenÃ§Ã£o.

---

## ğŸ§  O que Ã© Ollama?

Ollama (anteriormente Ollama) Ã© uma ferramenta que permite rodar modelos de linguagem grandes (LLMs) localmente no seu computador.

Site oficial: [https://ollama.com](https://ollama.com)

---

## âš™ï¸ Requisitos

- Python 3.10+
- Um computador com performance razoÃ¡vel
  - Preferencialmente com **GPU**
- EspaÃ§o livre em disco (modelos podem ocupar de 3 a 10GB)
- Ollama instalado

---

## ğŸš€ Como Instalar e Rodar

### 1. Clone este repositÃ³rio

```bash
git clone https://github.com/seu-usuario/chatbot-oylama.git
cd chatbot-oylama
```

### 2. Crie e ative um ambiente virtual (opcional, mas recomendado)

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

### 3. Instale as dependÃªncias Python

```bash
pip install -r requirements.txt
```

VocÃª pode criar o arquivo `requirements.txt` com:

```txt
langchain
langchain-core
langchain-community
langchain-ollama
```

### 4. Instale o Ollama

1. Acesse [https://ollama.com](https://ollama.com)
2. Baixe o instalador e siga os passos na tela.
3. ApÃ³s instalar, execute no terminal:

```bash
ollama run llama3
```

Isso farÃ¡ o download e inicializaÃ§Ã£o do modelo **LLaMA 3**.

### 5. Execute o chatbot

```bash
python main.py
```

---

## ğŸ§© Estrutura do Projeto

```bash
chatbot-oylama/
â”œâ”€â”€ chatbot/
â”‚   â”œâ”€â”€ chatbot.py               # LÃ³gica principal do bot
â”‚   â”œâ”€â”€ conversation_storage.py  # Armazena e carrega conversas
â”‚   â”œâ”€â”€ llm_service.py           # ComunicaÃ§Ã£o com o modelo LLM
â”‚   â””â”€â”€ prompt_builder.py        # ConstrÃ³i o prompt para o modelo
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ conversations/           # Onde as conversas sÃ£o salvas
â”œâ”€â”€ main.py                      # Entrada principal do projeto
â”œâ”€â”€ requirements.txt             # DependÃªncias Python
â””â”€â”€ README.md
```

---

## ğŸ’¬ Como Funciona

1. Ao rodar o programa, vocÃª escolhe um usuÃ¡rio ou cria um novo.
2. Escolhe iniciar uma nova conversa ou continuar uma antiga.
3. Digita sua pergunta e recebe uma resposta da IA.
4. Todo o histÃ³rico Ã© salvo automaticamente em arquivos `.json`.

---

## ğŸ§ª Exemplo de Uso

```bash
python main.py

ğŸ¤– Bem-vindo!
1. Selecionar usuÃ¡rio existente
2. Criar novo usuÃ¡rio
> 2

Novo usuÃ¡rio criado: user_abcd1234

1. Iniciar nova conversa
> 1

You: O que Ã© inteligÃªncia artificial?
Bot: InteligÃªncia artificial Ã© o campo da ciÃªncia...
```

---

## â“ DÃºvidas Frequentes

**1. Posso usar outro modelo alÃ©m do LLaMA 3?**  
Sim! Basta alterar o nome do modelo na classe `LLMService`:

```python
class LLMService:
    def __init__(self, model_name: str = "mistral"):
        ...
```

Depois, rode no terminal:

```bash
ollama run mistral
```

**2. E se minha mÃ¡quina for fraca?**  
VocÃª pode testar modelos mais leves como:

```bash
ollama run llama2
ollama run gemma
```

---

## ğŸ›  SugestÃµes Futuras

- Interface grÃ¡fica com Tkinter ou PyQt.
- API web com FastAPI.
- Suporte a mÃºltiplos modelos selecionÃ¡veis na inicializaÃ§Ã£o.

---

## ğŸ™‹â€â™‚ï¸ Autor

Feito com dedicaÃ§Ã£o e cÃ³digo limpo por Luis Gabriel GoÃ©s.

---
